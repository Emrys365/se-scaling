# DEMUCS-v4 xlarge: 79.3 M parameters
optim: adam
init: none
unused_parameters: true
max_epoch: 100
batch_type: folded
batch_size: 4
iterator_type: chunk
chunk_length: 32000
chunk_discard_short_samples: false
num_iters_per_epoch: 8000
num_workers: 4
grad_clip: 5.0
optim_conf:
    lr: 3.0e-04
    eps: 1.0e-08
    weight_decay: 0
patience: 10
val_scheduler_criterion:
- valid
- loss
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 1
scheduler: warmupReducelronplateau
scheduler_conf:
  # for WarmupLR
  warmup_steps: 4000
  # for ReduceLR
  mode: min
  factor: 0.5
  patience: 3

allow_multi_rates: true

preprocessor: enh
channel_reordering: true
force_single_channel: true
# The categories list order must be the same everywhere in this config
categories:
- 1ch_8k
- 1ch_8k_r
- 1ch_16k_r
- 1ch_48k
- 1ch_24k
- 1ch_16k
- 2ch_8k
- 2ch_8k_r
- 2ch_16k
- 2ch_16k_r
- 5ch_8k
- 5ch_16k
- 8ch_8k_r
- 8ch_16k_r
num_spk: 1

model_conf:
    normalize_variance_per_ch: true
    always_forward_in_48k: true
    # The categories list order must be the same everywhere in this config
    categories:
    - 1ch_8k
    - 1ch_8k_r
    - 1ch_16k_r
    - 1ch_48k
    - 1ch_24k
    - 1ch_16k
    - 2ch_8k
    - 2ch_8k_r
    - 2ch_16k
    - 2ch_16k_r
    - 5ch_8k
    - 5ch_16k
    - 8ch_8k_r
    - 8ch_16k_r

#====================================================
# Model configuration
#====================================================
encoder: same
decoder: same
separator: demucsv3
separator_conf:
  sources: [speech]
  nfft: 4096                # FFT size in STFT/iSTFT
  channels: 64              # initial number of hidden channels
  depth: 4                  # number of encoder/decoder layers
  t_layers: 10              # number of layers in each branch (waveform and spec) of the transformer
  time_stride: 2            # stride for the final time layer, after the merge
  stride: 4                 # stride for encoder and decoder layers
  samplerate: 48000         # sampling frequency that the model operates on
  segment: 4                # segment length for training (in seconds), unused when use_train_segment=False
  use_train_segment: false  # if True, the actual size that is used during training is used during inference

# A list for criterions
# The overlall loss in the multi-task learning will be:
# loss = weight_1 * loss_1 + ... + weight_N * loss_N
# The default `weight` for each sub-loss is 1.0
criterions:
  # The first criterion
  - name: mr_l1_tfd
    conf:
      window_sz: [256, 512, 768, 1024]
      hop_sz: null
      eps: 1.0e-8
      time_domain_weight: 0.5
      normalize_variance: true
      use_builtin_complex: true
    wrapper: fixed_order
    wrapper_conf:
      weight: 1.0
  # The second criterion
  - name: si_snr
    conf:
      eps: 1.0e-7
    wrapper: fixed_order
    wrapper_conf:
      weight: 0.0
